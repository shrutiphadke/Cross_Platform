{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code preprocesses already created entity names in NERecog.ipynb and creates lists of merged entities and documents. Then replaces the multi word phrases by joining them with \"_\". \n",
    "### This then outputs entity_vocab, entity_corpus and entity_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build complete vocabulary\n",
    "\n",
    "vocab = []\n",
    "\n",
    "with open(\"/home/rin/CrossPlatform/Code/Entity_Vocab/Only_PROPN/Gab_vocab.txt\", 'r') as gfile:\n",
    "    for line in gfile:\n",
    "        word = str(line.strip())\n",
    "        if len(word) > 4:\n",
    "            vocab.append(word)\n",
    "        \n",
    "with open(\"/home/rin/CrossPlatform/Code/Entity_Vocab/Only_PROPN/Twitter_vocab.txt\", 'r') as gfile:\n",
    "    for line in gfile:\n",
    "        word = str(line.strip())\n",
    "        if len(word) > 4:\n",
    "            vocab.append(word)\n",
    "\n",
    "        \n",
    "with open(\"/home/rin/CrossPlatform/Code/Entity_Vocab/Only_PROPN/Facebook_vocab.txt\", 'r') as gfile:\n",
    "    for line in gfile:\n",
    "        word = str(line.strip())\n",
    "        if len(word) > 4:\n",
    "            vocab.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(vocab))\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/rin/CrossPlatform/Code/entity_vocab_onlyPROPN', 'wb') as pfile:\n",
    "    pickle.dump(vocab, pfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create document corpus\n",
    "\n",
    "#paths\n",
    "# Twitter path\n",
    "tp = '/home/rin/CrossPlatform/raw_text/Twitter/'\n",
    "\n",
    "#Facebook path\n",
    "fp = '/home/rin/CrossPlatform/raw_text/Facebook/'\n",
    "\n",
    "#Gab path\n",
    "gp = '/home/rin/CrossPlatform/raw_text/Gab/'\n",
    "\n",
    "extremists = [\"FrankGaffney.txt\", \"LarryPratt.txt\", \"PaulRamsey.txt\", \"StefanMolynuex.txt\", \"JosephFarah.txt\", \"MikeCernovich.txt\", \"PeterBrimelow.txt\" ]\n",
    "ideologies = [\"antiMuslim\", \"antiGov\", \"whiteNat\", \"altRight\", \"antiGov\", \"maleSupr\", \"whiteNat\"]\n",
    "\n",
    "#documents\n",
    "documents = []\n",
    "folders = [tp, fp, gp]\n",
    "\n",
    "for fold in folders:\n",
    "    for extreme in extremists:\n",
    "        with open(fold+extreme, 'r') as rfile:\n",
    "            for line in rfile:\n",
    "                if len(line) > 5:\n",
    "                    documents.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81637\n"
     ]
    }
   ],
   "source": [
    "print len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['terms', 'and', 'conditions']\n",
      "terms_and_conditions\n"
     ]
    }
   ],
   "source": [
    "#Join phrases in the vocab\n",
    "x = \"terms and conditions\"\n",
    "print x.split(\" \")\n",
    "print \"_\".join(x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary for phrases and joined phrases\n",
    "translate_vocab = defaultdict()\n",
    "for v in vocab:\n",
    "    translate_vocab[v] = \"_\".join(v.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from text_cleaner import remove, keep\n",
    "\n",
    "from text_cleaner.processor.common import ASCII\n",
    "from text_cleaner.processor.chinese import CHINESE, CHINESE_SYMBOLS_AND_PUNCTUATION\n",
    "from text_cleaner.processor.misc import RESTRICT_URL\n",
    "\n",
    "def cleaning2(text):\n",
    "    text = remove(text, [RESTRICT_URL])\n",
    "    text = re.sub(r'\\b(?:(?:https?|ftp)://)?\\w[\\w-]*(?:\\.[\\w-]+)+\\S*(?<![.,])', ' ', text.lower())\n",
    "    words = re.findall(r'[a-z.,]+', text)\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    test_str = cleaning2(documents[i])\n",
    "    for v in vocab:\n",
    "        if v in test_str:\n",
    "            test_str = test_str.replace(v, translate_vocab[v])\n",
    "            \n",
    "    documents[i] = test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/rin/CrossPlatform/Code/entity_corpus_onlyPROPN\", \"wb\") as dfile:\n",
    "    pickle.dump(documents, dfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/rin/CrossPlatform/Code/entity_vocab_onlyPROPN\", 'r') as vfile:\n",
    "    vcb = pickle.load(vfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump dictionaries also\n",
    "with open('/home/rin/CrossPlatform/Code/entity_dict_onlyPROPN', 'wb') as handle:\n",
    "    pickle.dump(translate_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
