{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter path\n",
    "tp = '/home/rin/CrossPlatform/raw_text/Twitter/'\n",
    "\n",
    "#Facebook path\n",
    "fp = '/home/rin/CrossPlatform/raw_text/Facebook/'\n",
    "\n",
    "#Gab path\n",
    "gp = '/home/rin/CrossPlatform/raw_text/Gab/'\n",
    "\n",
    "extremists = [\"FrankGaffney.txt\", \"LarryPratt.txt\", \"PaulRamsey.txt\", \"StefanMolynuex.txt\", \"JosephFarah.txt\", \"MikeCernovich.txt\", \"PeterBrimelow.txt\" ]\n",
    "ideologies = [\"antiMuslim\", \"antiGov\", \"whiteNat\", \"altRight\", \"antiGov\", \"maleSupr\", \"whiteNat\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    " \n",
    "sentence = \"From the Soviet Union to Cuba to Venezuela, wherever true socialism or communism has been adopted, it has delivered anguish and devastation and failure.  Those who preach the tenets of these discredited ideologies only contribute to the continued suffering of the people who live under these cruel systems.  Trump at the UN 9/19/17\"\n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    " \n",
    "iob_tagged = tree2conlltags(ne_tree)\n",
    "ne_tree = conlltags2tree(iob_tagged)\n",
    "#print ne_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Musab, u'Musab', u'PROPN', u'NNP', u'compound', u'Xxxxx', True, False)\n",
      "(Yousef, u'Yousef', u'PROPN', u'NNP', u'npadvmod', u'Xxxxx', True, False)\n",
      "(,, u',', u'PUNCT', u',', u'punct', u',', False, False)\n",
      "(ex-Hamas militant, u'ex', u'NOUN', u'NN', u'appos', u'xx-Xxxxx xxxx', False, False)\n",
      "(:, u':', u'PUNCT', u':', u'punct', u':', False, False)\n",
      "(Islam, u'Islam', u'PROPN', u'NNP', u'appos', u'Xxxxx', True, False)\n",
      "(the biggest lie, u'the', u'NOUN', u'NN', u'dobj', u'xxx xxxx xxx', False, False)\n",
      "(in, u'in', u'ADP', u'IN', u'prep', u'xx', True, True)\n",
      "(history, u'history', u'NOUN', u'NN', u'pobj', u'xxxx', True, False)\n",
      "( , u' ', u'SPACE', u'_SP', u'', u' ', False, False)\n",
      "(and, u'and', u'CCONJ', u'CC', u'cc', u'xxx', True, True)\n",
      "(Allah, u'Allah', u'PROPN', u'NNP', u'conj', u'Xxxxx', True, False)\n",
      "(the biggest terrorist, u'the', u'NOUN', u'NN', u'appos', u'xxx xxxx xxxx', False, False)\n",
      "(while, u'while', u'ADP', u'IN', u'mark', u'xxxx', True, True)\n",
      "(saying, u'say', u'VERB', u'VBG', u'advcl', u'xxxx', True, False)\n",
      "(,, u',', u'PUNCT', u',', u'punct', u',', False, False)\n",
      "(only Jesus, u'only', u'PROPN', u'NNP', u'nsubj', u'xxxx Xxxxx', False, False)\n",
      "(could, u'could', u'VERB', u'MD', u'aux', u'xxxx', True, True)\n",
      "(save, u'save', u'VERB', u'VB', u'ROOT', u'xxxx', True, False)\n",
      "(mankind, u'mankind', u'NOUN', u'NN', u'dobj', u'xxxx', True, False)\n",
      "(and, u'and', u'CCONJ', u'CC', u'cc', u'xxx', True, True)\n",
      "(that, u'that', u'ADP', u'IN', u'mark', u'xxxx', True, True)\n",
      "(Jesus, u'Jesus', u'PROPN', u'NNP', u'nsubj', u'Xxxxx', True, False)\n",
      "(is, u'be', u'VERB', u'VBZ', u'conj', u'xx', True, True)\n",
      "(the only way, u'the', u'NOUN', u'NN', u'attr', u'xxx xxxx xxx', False, False)\n",
      "(to, u'to', u'ADP', u'IN', u'prep', u'xx', True, True)\n",
      "(God, u'God', u'PROPN', u'NNP', u'pobj', u'Xxx', True, False)\n",
      "(., u'.', u'PUNCT', u'.', u'punct', u'.', False, False)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "texts = [t.text for t in nlp(u\"I have a blue car\")]\n",
    "#print texts\n",
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "\n",
    "#texts = [t.text for t in nlp(u\"Musab Yousef, ex-Hamas militant: Islam the biggest lie in history  and Allah the biggest terrorist while saying, only Jesus could save mankind and that Jesus is the only way to God.\")]\n",
    "#print texts\n",
    "\n",
    "texts = nlp(u\"Musab Yousef, ex-Hamas militant: Islam the biggest lie in history  and Allah the biggest terrorist while saying, only Jesus could save mankind and that Jesus is the only way to God.\")\n",
    "for token in texts:\n",
    "    print(token, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "#tokens = nlp.tokenizer.tokens_from_list(texts)\n",
    "#tags = nlp.tagger(tokens)\n",
    "#print tokens\n",
    "#print \"fif\" , tags, \"xyzp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from spacy import displacy\n",
    "displacy.serve(texts, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from text_cleaner import remove, keep\n",
    "\n",
    "from text_cleaner.processor.common import ASCII\n",
    "from text_cleaner.processor.chinese import CHINESE, CHINESE_SYMBOLS_AND_PUNCTUATION\n",
    "from text_cleaner.processor.misc import RESTRICT_URL\n",
    "\n",
    "def cleaning2(text):\n",
    "    text = remove(text, [RESTRICT_URL])\n",
    "    text = re.sub(r'\\b(?:(?:https?|ftp)://)?\\w[\\w-]*(?:\\.[\\w-]+)+\\S*(?<![.,])', ' ', text.lower())\n",
    "    words = re.findall(r'[a-z.,]+', text)\n",
    "    #return text\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of Twitter Proper Nouns\n",
    "import spacy\n",
    "\n",
    "#nlp = spacy.load('en')\n",
    "nlp = spacy.load('en')\n",
    "#print texts\n",
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "gents = []\n",
    "\n",
    "for extreme in extremists:\n",
    "    fname = gp + extreme\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            #print line, \"-----------------------\"\n",
    "            sentence =  unicode(cleaning2(line))\n",
    "            #sentence = unicode(line)\n",
    "            #print sentence\n",
    "            try:\n",
    "                texts = nlp(sentence)\n",
    "                for token in texts:\n",
    "                    if token.pos_==\"PROPN\": #or token.pos_==\"NOUN\":\n",
    "                        gents.append(token)\n",
    "            except:\n",
    "                fakevar=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[california, steele, goc, @army8488 @nra, @gregpruett2aa, @gregpruett2aa, santa, santa, @whereisbullet, @whereisbullet, the goa-backed @rondesantisfl, california, @whereisbullet, i-1639, goups+anti-2a, @whereisbullet, @iraqveteran8888, @iraqveteran8888, june, msnbc, july, chica, wednesday, jfk, @cernovich, @realalexjones, @voxday, zuckerberg, july, @cernovich, |, |, |, july, @cernovich, @scrowder, clinton, @voxday, |, hillary clinton, @voxday, @prisonplanet, @voxday, @chuckcjohnson, @cernovich, @gavinmcinnes, @realalexjones, |, @voxday, al, holy s%!t, live stream q&a, h-1b, jesus, hillary clinton, |, bashar al-assad, styxhexenhammer666, |, @prisonplanet, paris, hillary, hillary, @pnehlen, andrew, muslim, president, @batmaniac7, kate steinle's dad jim, democrat, @cernovich, washington, washington, @realalexjones, @cernovich, iran, sweden, dar al-islam, dar al-islam, @cernovich, eu, london, @josephfarah, jesus, july, jim, caitlyn jenner, muslim, democrat, allen, top democrat, ex-president, july, english, sick hillary clinton, sick hillary clinton, @voxday, @voxday, @gae, clinton, christmas, @voxday, hillary, google, @voxday, vox, vox, @voxday, jews, bmw, this event himself,, it--\"in, @walkerstapleton, strictly speaking, re-elect, @richardbspencer, @steve_sailer, the j-20, john]\n"
     ]
    }
   ],
   "source": [
    "print gents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of Gab proper nouns\n",
    "nlp = spacy.load('en')\n",
    "#print texts\n",
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "gents = []\n",
    "\n",
    "for extreme in extremists:\n",
    "    fname = gp + extreme\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            #print line, \"-----------------------\"\n",
    "            sentence =  unicode(cleaning2(line))\n",
    "            try:\n",
    "                texts = nlp(sentence)\n",
    "                for token in texts:\n",
    "                    if token.pos_==\"PROPN\":# or token.pos_==\"NOUN\":\n",
    "                        gents.append(token)\n",
    "            except:\n",
    "                fakevar=1\n",
    "            \n",
    "            \n",
    "\n",
    "#Create a list of Twitter Proper Nouns\n",
    "nlp = spacy.load('en')\n",
    "#print texts\n",
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "tents = []\n",
    "\n",
    "for extreme in extremists:\n",
    "    fname = tp + extreme\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            #print line, \"-----------------------\"\n",
    "            sentence =  unicode(cleaning2(line))\n",
    "            try:\n",
    "                texts = nlp(sentence)\n",
    "                for token in texts:\n",
    "                    if token.pos_==\"PROPN\":# or token.pos_==\"NOUN\":\n",
    "                        tents.append(token)\n",
    "            except:\n",
    "                fakevar=1\n",
    "            \n",
    "\n",
    "#Create a list of Facebook Proper Nouns\n",
    "nlp = spacy.load('en')\n",
    "#print texts\n",
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "fents = []\n",
    "\n",
    "for extreme in extremists:\n",
    "    fname = fp + extreme\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            #print line, \"-----------------------\"\n",
    "            sentence =  unicode(cleaning2(line))\n",
    "            try:\n",
    "                texts = nlp(sentence)\n",
    "                for token in texts:\n",
    "                    if token.pos_==\"PROPN\":# or token.pos_==\"NOUN\":\n",
    "                        fents.append(token)\n",
    "            except:\n",
    "                fakevar=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write vocabulary to files\n",
    "#Gab entity vocab\n",
    "gents = [str(g) for g in gents]\n",
    "gents = list(set(gents))\n",
    "with open(\"/home/rin/CrossPlatform/Code/Entity_Vocab/Only_PROPN/Gab_vocab.txt\", 'w') as gfile:\n",
    "    for g in gents:\n",
    "        gfile.write(g)\n",
    "        gfile.write('\\n')\n",
    "        \n",
    "\n",
    "#Twitter entity vocab\n",
    "tents = [str(t) for t in tents]\n",
    "tents = list(set(tents))\n",
    "with open(\"/home/rin/CrossPlatform/Code/Entity_Vocab/Only_PROPN/Twitter_vocab.txt\", 'w') as tfile:\n",
    "    for t in tents:\n",
    "        tfile.write(t)\n",
    "        tfile.write('\\n')\n",
    "        \n",
    "        \n",
    "#Facebook entity vocab\n",
    "fents = [str(f) for f in fents]\n",
    "fents = list(set(fents))\n",
    "with open(\"/home/rin/CrossPlatform/Code/Entity_Vocab/Only_PROPN/Facebook_vocab.txt\", 'w') as ffile:\n",
    "    for f in fents:\n",
    "        ffile.write(f)\n",
    "        ffile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tents = [str(t) for t in gents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "print len(tents)\n",
    "print len(list(set(tents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kate steinle s dad jim', 'google', 'clinton', 've', 'nd', 'ann', 'washington', 'al', 'democrat', 'jews', 'london', 'eu', 'allen', 'vox', 'this event himself,', 'jesus', 'pat', 'paris', 'jim', 'xoxo', 'sweden', 'john', 'christmas', 'sick hillary clinton', 'mike', 'muslim', 'josephfarah', 'andrew', 'wednesday', 'hillary', 'bible', 'california', 'bmw', 'santa', 'obama s island paradise', 'president', 'july', 'media hoax sick hillary clinton', 'detective top democrat', 'i', 'hillary clinton', 'caitlyn jenner', 'american', 'chica', 'english', 'sb']\n"
     ]
    }
   ],
   "source": [
    "print list(set(tents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
